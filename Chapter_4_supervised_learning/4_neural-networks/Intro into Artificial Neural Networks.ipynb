{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "### for classification\n",
    "\n",
    "<img src='../../images/ANN.png'>\n",
    "\n",
    "\n",
    "### for regression\n",
    "\n",
    "<img src='../../images/regann.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "`Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction. Activation functions also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1`\n",
    "\n",
    "## most common types\n",
    "\n",
    "\n",
    "<img src='../../images/sigmoid.gif'>\n",
    "\n",
    "`\n",
    "there is popular problem with sigmoid , where it saturates for large positive or negative values , this cause the problem of vanishing gradient as saturation indicates zero or very small change , so another activation function \n",
    "(RELU) solves this problem , One major benefit is the reduced likelihood of the gradient to vanish. This arises when its input > 0. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.\n",
    "`\n",
    "<img src='../../images/relu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Step >>> forward propagation\n",
    "\n",
    "\n",
    "<img src='../../images/forward.png'>\n",
    "\n",
    "### Second step <<<<<< backpropagation\n",
    "\n",
    "<img src='../../images/backw1.png'>\n",
    "\n",
    "<img src='../../images/backw2.png'>\n",
    "\n",
    "### Third step : \n",
    "\n",
    "`keep update weights until gradient descent reach global minimum`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning in Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
